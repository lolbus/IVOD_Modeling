{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYN3k0ASS0Nr",
        "outputId": "1beffc11-82bb-48a2-db19-dbff6525f0c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16YMR5gvL1tU6Z3_NoJ-u-uaujqS9XTYx\n",
            "To: /content/IVOD_Modeling.zip\n",
            "100% 8.68M/8.68M [00:00<00:00, 22.8MB/s]\n",
            "Collecting zipfile-deflate64\n",
            "  Downloading zipfile_deflate64-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zipfile-deflate64\n",
            "Successfully installed zipfile-deflate64-0.2.0\n"
          ]
        }
      ],
      "source": [
        "# Download necessary folders and compressor\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Download and setup ivod modeling source code\n",
        "\n",
        "\n",
        "# https://drive.google.com/file/d/1HGFNpUzk1qhBtI-yhB9GkY3AyqE_BUfQ/view?usp=sharing\n",
        "# !gdown --no-cookies https://drive.google.com/uc?id=1HGFNpUzk1qhBtI-yhB9GkY3AyqE_BUfQ # Download IVOD AI Project source code\n",
        "\n",
        "# New IVOD AI Project source code (June 10 2023)\n",
        "# https://drive.google.com/file/d/1SIqHxh9ffH0rSiBWH6XKpUtVBS-zlrjS/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1SIqHxh9ffH0rSiBWH6XKpUtVBS-zlrjS/view?usp=sharing\n",
        "\n",
        "#!gdown --no-cookies https://drive.google.com/uc?id=1SIqHxh9ffH0rSiBWH6XKpUtVBS-zlrjS\n",
        "\n",
        "# Latest as of 26 June 23\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=16YMR5gvL1tU6Z3_NoJ-u-uaujqS9XTYx\n",
        "!pip install zipfile-deflate64\n",
        "\n",
        "import zipfile_deflate64 as zipfile64\n",
        "import os\n",
        "\n",
        "# directory where the files will be extracted\n",
        "extract_to_dir = \"./\"\n",
        "\n",
        "# create the directory if it doesn't exist\n",
        "if not os.path.exists(extract_to_dir):\n",
        "    os.makedirs(extract_to_dir)\n",
        "\n",
        "\n",
        "destination7 = './IVOD_Modeling.zip' #directory to unzip IVOD AI Project\n",
        "# extract all the files in the zip archive IVOD AI Project\n",
        "with zipfile64.ZipFile(destination7, \"r\") as zip_file:\n",
        "    zip_file.extractall(extract_to_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aQ2aAx24baG",
        "outputId": "1a234255-407a-4c23-ac4f-b3d45cb7bedb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IP6zs2U3QJuhWs6A5htFVOBdGJtq2gPm\n",
            "To: /content/PassengerNo_0 (Empty).zip\n",
            "100% 17.3M/17.3M [00:00<00:00, 91.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CyWParUnk2y-w5FH8GpgrQgs8xEAAN1-\n",
            "To: /content/PassengerNo_1 (D).zip\n",
            "100% 2.62G/2.62G [00:24<00:00, 105MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1u703c8SatoQfuhjlNi7b7vxd_nCbp0Pc\n",
            "To: /content/PassengerNo_2 (D+FP).zip\n",
            "100% 3.67G/3.67G [00:41<00:00, 87.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LE51GGX2SVxonN9r47NahTr5H5Ha102D\n",
            "To: /content/PassengerNo_2 (D+LB).zip\n",
            "100% 3.81G/3.81G [00:40<00:00, 94.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1G3IhSPgxS8X_rhsnMDB8f4mf_UoYo39n\n",
            "To: /content/PassengerNo_3 (D+FP+LB).zip\n",
            "100% 2.29G/2.29G [00:24<00:00, 94.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=165982FDPFFSjnn8zUP-imxd4h6I3EjPn\n",
            "To: /content/PassengerNo_0 (Empty)2.zip\n",
            "100% 162k/162k [00:00<00:00, 68.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LXi_Up8xcczAv2PbAH8snAbmL53aqLTD\n",
            "To: /content/PassengerNo_3 (D+FP+LB)(C).zip\n",
            "100% 297M/297M [00:08<00:00, 36.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-9gmXZyI2fx-SNBIBXKgjqITRLKeIGrX\n",
            "To: /content/PassengerNo_2 (D+FP)(C).zip\n",
            "100% 166M/166M [00:04<00:00, 34.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fV0Z9c8nwoM2KECwHlD9yjF7v8S3UKzh\n",
            "To: /content/PassengerNo_1 (D)(C).zip\n",
            "100% 99.5M/99.5M [00:01<00:00, 52.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download dataset\n",
        "\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1IP6zs2U3QJuhWs6A5htFVOBdGJtq2gPm # Empty\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1CyWParUnk2y-w5FH8GpgrQgs8xEAAN1- # D\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1u703c8SatoQfuhjlNi7b7vxd_nCbp0Pc # D+FP\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1LE51GGX2SVxonN9r47NahTr5H5Ha102D # D+LB\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1G3IhSPgxS8X_rhsnMDB8f4mf_UoYo39n # D+FP+LB\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=165982FDPFFSjnn8zUP-imxd4h6I3EjPn # Empty2\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1LXi_Up8xcczAv2PbAH8snAbmL53aqLTD # D+FP+LB (C)\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1-9gmXZyI2fx-SNBIBXKgjqITRLKeIGrX # D+FP (C)\n",
        "!gdown --no-cookies https://drive.google.com/uc?id=1fV0Z9c8nwoM2KECwHlD9yjF7v8S3UKzh # D (C)\n",
        "\n",
        "# !gdown --no-cookies https://drive.google.com/uc?id=1wsDSfWRDp0k7kxKrTw6HPlB1J5NHriPM # D\n",
        "# !gdown --no-cookies https://drive.google.com/uc?id=1ipwOIH6akxzmSHI4ojwgIKJf0HQz5ued # D+FP\n",
        "# !gdown --no-cookies https://drive.google.com/uc?id=1w3qp0umUczchLFQwDvncsXit2E-xfUTC # D+LB\n",
        "# !gdown --no-cookies https://drive.google.com/uc?id=10lFErzWSbRH2c6woHWwOLQ58AJdKm3Q_ # D+FP+LB\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDGcj4d1QH4M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "bbc02036-ff59-4107-bb0c-085f1b99c41d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: zipfile-deflate64 in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# extract all the files in the zip archive for Class Empty\\nwith zipfile.ZipFile(destination0, \"r\") as zip_file:\\n    zip_file.extractall(extract_to_dir)\\n\\n\\n# extract all the files in the zip archive for Class D\\nwith zipfile.ZipFile(destination1, \"r\") as zip_file:\\n    zip_file.extractall(extract_to_dir)\\n\\n# extract all the files in the zip archive D+FP\\nwith zipfile.ZipFile(destination2, \"r\") as zip_file:\\n    zip_file.extractall(extract_to_dir)\\n    # extract all the files in the zip archive\\n\\n# Folder 3 and 4 (D+LB and D+FP+LB) use a special different compression method that need a special zipfile lib\\n# extract all the files in the zip archive D+LB\\nwith zipfile64.ZipFile(destination3, \"r\") as zip_file:\\n    zip_file.extractall(extract_to_dir)\\n\\n# extract all the files in the zip archive D+FP+LB\\nwith zipfile64.ZipFile(destination4, \"r\") as zip_file:\\n    zip_file.extractall(extract_to_dir)\\n\\n# extract all the files in the zip archive Empty 2\\nwith zipfile64.ZipFile(destination5, \"r\") as zip_file:\\n    zip_file.extractall(extract_to_dir)\\n\\n# extract all the files in the zip archive D+FP+LB(C)\\nwith zipfile64.ZipFile(destination6, \"r\") as zip_file:\\n    zip_file.extractall(extract_to_dir)\\n\\n\\n# extract all the files in the zip archive D+FP+LB(C)\\nwith zipfile64.ZipFile(destination7, \"r\") as zip_file:\\n    zip_file.extractall(extract_to_dir)\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Unzipping dowmloads\n",
        "\n",
        "# Destination to save\n",
        "'''destination0 = './PassengerNo_0 (Empty).zip'\n",
        "destination1 = './PassengerNo_1 (D).zip'\n",
        "destination2 = './PassengerNo_2 (D+FP).zip'\n",
        "destination3 = './PassengerNo_2 (D+LB).zip'\n",
        "destination4 = './PassengerNo_3 (D+FP+LB).zip'\n",
        "destination5 = './PassengerNo_0 (Empty)2.zip'\n",
        "destination6 = './PassengerNo_3 (D+FP+LB)(C).zip'\n",
        "destination7 = './PassengerNo_2 (D+FP)(C).zip'''\n",
        "\n",
        "destination_list = ['./PassengerNo_0 (Empty).zip',\n",
        "                    './PassengerNo_1 (D).zip',\n",
        "                    './PassengerNo_2 (D+FP).zip',\n",
        "                    './PassengerNo_2 (D+LB).zip',\n",
        "                    './PassengerNo_3 (D+FP+LB).zip',\n",
        "                    './PassengerNo_0 (Empty)2.zip',\n",
        "                    './PassengerNo_3 (D+FP+LB)(C).zip',\n",
        "                    './PassengerNo_2 (D+FP)(C).zip',\n",
        "                    './PassengerNo_1 (D)(C).zip'\n",
        "                    ]\n",
        "\n",
        "\n",
        "!pip install zipfile-deflate64\n",
        "import zipfile\n",
        "import zipfile_deflate64 as zipfile64\n",
        "import os\n",
        "\n",
        "\n",
        "# directory where the files will be extracted\n",
        "extract_to_dir = \"./\"\n",
        "\n",
        "# create the directory if it doesn't exist\n",
        "if not os.path.exists(extract_to_dir):\n",
        "    os.makedirs(extract_to_dir)\n",
        "\n",
        "for destination in destination_list:\n",
        "    with zipfile.ZipFile(destination, \"r\") as zip_file:\n",
        "        zip_file.extractall(extract_to_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBsGd9fAS0Xf",
        "outputId": "81cb0e28-7fec-421d-9bcd-70fa4d5113f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "List object loaded successfully. Total blacklisted folders: 0\n",
            "Loaded List: []\n",
            "Total bad data: 0. Time taken to extract data location: 0.003593921661376953s\n",
            "preparing 5000 instances for class PassengerNo_0 (Empty)\n",
            "completed loading for class PassengerNo_0 (Empty) [0, 0, 0]. Total instances 5001\n",
            "preparing 200 instances for class PassengerNo_1 (D)\n",
            "Finished processing 200 data instances. Current len 5200\n",
            "completed loading for class PassengerNo_1 (D) [1, 0, 0]. Total instances 5200\n",
            "preparing 200 instances for class PassengerNo_2 (D+FP)\n",
            "Finished processing 200 data instances. Current len 5399\n",
            "completed loading for class PassengerNo_2 (D+FP) [1, 1, 0]. Total instances 5399\n",
            "preparing 1000 instances for class PassengerNo_2 (D+LB)\n",
            "Bad data detected!!![7687, 'PassengerNo_2 (D+LB)', './/PassengerNo_2 (D+LB)/7687']. count 448 foldername 7687\n",
            "Finished processing 1000 data instances. Current len 6397\n",
            "completed loading for class PassengerNo_2 (D+LB) [1, 0, 1]. Total instances 6397\n",
            "preparing 0 instances for class PassengerNo_3 (D+FP+LB)\n",
            "Finished processing 0 data instances. Current len 6397\n",
            "completed loading for class PassengerNo_3 (D+FP+LB) [1, 1, 1]. Total instances 6397\n",
            "preparing 0 instances for class PassengerNo_0 (Empty)2\n",
            "Finished processing 0 data instances. Current len 6397\n",
            "completed loading for class PassengerNo_0 (Empty)2 [0, 0, 0]. Total instances 6397\n",
            "preparing 500 instances for class PassengerNo_3 (D+FP+LB)(C)\n",
            "completed loading for class PassengerNo_3 (D+FP+LB)(C) [1, 1, 1]. Total instances 6898\n",
            "preparing 500 instances for class PassengerNo_2 (D+FP)(C)\n",
            "completed loading for class PassengerNo_2 (D+FP)(C) [1, 1, 0]. Total instances 7399\n",
            "preparing 500 instances for class PassengerNo_2 (D+FP)(C2)\n",
            "completed loading for class PassengerNo_2 (D+FP)(C2) [1, 1, 0]. Total instances 7900\n",
            "preparing 2100 instances for class PassengerNo_1 (D)(C)\n",
            "completed loading for class PassengerNo_1 (D)(C) [1, 0, 0]. Total instances 10001\n",
            "Total data: 10001. Time taken to extract data location: 0.16226434707641602s\n",
            "Completed 0/3300\n",
            "Completed 1000/3300\n",
            "Completed 2000/3300\n",
            "Completed 3000/3300\n",
            "Bad data detected  []\n",
            "saving local\n",
            "Saved train_tensor success of shape torch.Size([3300, 3, 180, 298])\n",
            "Completed 0/3300\n",
            "Completed 1000/3300\n",
            "Check data format: 141 string indices must be integers\n",
            "No data received\n",
            "Skipping json object of line 141. Unusual circumstance. json object size 0 values: []\n",
            "Check data format: 266 string indices must be integers\n",
            "No data received\n",
            "Skipping json object of line 266. Unusual circumstance. json object size 0 values: []\n",
            "Check data format: 291 string indices must be integers\n",
            "No data received\n",
            "Skipping json object of line 291. Unusual circumstance. json object size 0 values: []\n",
            "Completed 2000/3300\n",
            "Completed 3000/3300\n",
            "Bad data detected  []\n",
            "Saved validation_tensor success of shape torch.Size([3300, 3, 180, 298])\n",
            "Completed 0/3401\n",
            "Check data format: 197 string indices must be integers\n",
            "No data received\n",
            "Skipping json object of line 197. Unusual circumstance. json object size 0 values: []\n",
            "Check data format: 215 string indices must be integers\n",
            "No data received\n",
            "Skipping json object of line 215. Unusual circumstance. json object size 0 values: []\n",
            "Check data format: 217 string indices must be integers\n",
            "No data received\n",
            "Skipping json object of line 217. Unusual circumstance. json object size 0 values: []\n",
            "Completed 1000/3401\n",
            "Check data format: 333 string indices must be integers\n",
            "No data received\n",
            "Skipping json object of line 333. Unusual circumstance. json object size 0 values: []\n",
            "Check data format: 354 string indices must be integers\n",
            "No data received\n",
            "Skipping json object of line 354. Unusual circumstance. json object size 0 values: []\n",
            "Completed 2000/3401\n",
            "Completed 3000/3401\n",
            "Bad data detected  []\n",
            "Saved validation_tensor success of shape torch.Size([3401, 3, 180, 298])\n",
            "Completed data processing for train/validation/test set, time taken 5925.203408002853\n",
            "./validation_dataset.csv is neither a valid file nor a folder.\n",
            "./train_dataset.csv is neither a valid file nor a folder.\n",
            "./test_dataset.csv is neither a valid file nor a folder.\n",
            "Preparing dataset done at 202308090152_\n",
            "File ./202308090152_dataset.zip has been copied to: /content/gdrive/MyDrive/IVODData\n",
            "File ./202308090152_befstand_heatmap.zip has been copied to: /content/gdrive/MyDrive/IVODData\n",
            "File ./202308090152_final.zip has been copied to: /content/gdrive/MyDrive/IVODData\n"
          ]
        }
      ],
      "source": [
        "# The main preprocess trigger code\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "import TrainingPreprocesser as tphandler\n",
        "import datahandler as dh\n",
        "import pandas as pd\n",
        "from metadata import DatasetMeta\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Import necessary libraries to copy output to gdrive\n",
        "import shutil\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "metadata = DatasetMeta()\n",
        "\n",
        "bad_data_dict = {\n",
        "    'file_path1': './folder1_bad_data.txt',\n",
        "    'file_path2': './folder2_bad_data.txt',\n",
        "    'file_path3': './folder3_bad_data.txt',\n",
        "    'file_path4': './folder4_bad_data.txt',\n",
        "    'file_path_devtest': './devtest.txt'\n",
        "    }\n",
        "\n",
        "# Retrieve list of bad data\n",
        "bad_dataset_id, load_bad_data_id_time_take = tphandler.load_bad_data(bad_data_dict)\n",
        "print(f'Total bad data: {len(bad_dataset_id)}. Time taken to extract data location: {load_bad_data_id_time_take}s')\n",
        "\n",
        "# Retrieve dataset location\n",
        "dataset_id_locations, load_data_id_time_take = tphandler.osdir_handler(\"./\", bad_dataset_id)\n",
        "print(f'Total data: {len(dataset_id_locations)}. Time taken to extract data location: {load_data_id_time_take}s')\n",
        "\n",
        "# Set the proportions for the split\n",
        "train_ratio = 0.33\n",
        "validation_ratio = 0.33\n",
        "test_ratio = 0.34\n",
        "train_ids, test_ids = train_test_split(dataset_id_locations, test_size=1 - train_ratio)\n",
        "validation_ids, test_ids = train_test_split(test_ids, test_size=test_ratio/(test_ratio + validation_ratio))\n",
        "\n",
        "# Begin data preprocessing for train set\n",
        "start= time.time()\n",
        "df, train_tensor, train_label_tensor = tphandler.training_set_preprocess(train_ids, augmentation = False)\n",
        "print(\"saving local\")\n",
        "df.to_csv('./p1_dataset.csv', index=False)\n",
        "torch.save(train_tensor, 'p1_X_tensor.pt')\n",
        "torch.save(train_label_tensor, 'p1_Y_tensor.pt')\n",
        "print(\"Saved train_tensor success of shape\", train_tensor.shape)\n",
        "del train_tensor\n",
        "\n",
        "# Begin data preprocessing for Validation/test set Save the ID used for devtest set so that future development can replicate datasets that omit these instances\n",
        "df_validation, validation_tensor, validation_label_tensor = tphandler.training_set_preprocess(validation_ids, augmentation = False)\n",
        "df_validation.to_csv('./p2_dataset.csv', index=False)\n",
        "torch.save(validation_tensor, 'p2_X_tensor.pt')\n",
        "torch.save(validation_label_tensor, 'p2_Y_tensor.pt')\n",
        "print(\"Saved validation_tensor success of shape\", validation_tensor.shape)\n",
        "del validation_tensor\n",
        "\n",
        "df_test, test_tensor, test_label_tensor = tphandler.training_set_preprocess(test_ids, augmentation = False)\n",
        "df_test.to_csv('./p3_dataset.csv', index=False)\n",
        "torch.save(test_tensor, 'p3_X_tensor.pt')\n",
        "torch.save(test_label_tensor, 'p3_Y_tensor.pt')\n",
        "print(\"Saved validation_tensor success of shape\", test_tensor.shape)\n",
        "del test_tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def zip_files_and_folders(filenames_and_folders, zip_name):\n",
        "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for file_or_folder in filenames_and_folders:\n",
        "            if os.path.isfile(file_or_folder):\n",
        "                # If it is a file, write it directly.\n",
        "                zipf.write(file_or_folder)\n",
        "            elif os.path.isdir(file_or_folder):\n",
        "                # If it is a directory, write all its contents.\n",
        "                for root, dirs, files in os.walk(file_or_folder):\n",
        "                    for file in files:\n",
        "                        # This will include the full directory structure\n",
        "                        full_path = os.path.join(root, file)\n",
        "                        relative_path = os.path.relpath(full_path, os.path.dirname(file_or_folder))\n",
        "                        zipf.write(full_path, arcname=relative_path)\n",
        "            else:\n",
        "                print(f\"{file_or_folder} is neither a valid file nor a folder.\")\n",
        "\n",
        "def zip_folders(filenames_and_folders, zip_name):\n",
        "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for file_or_folder in filenames_and_folders:\n",
        "            # If it is a directory, write all its contents.\n",
        "            for root, dirs, files in os.walk(file_or_folder):\n",
        "                for file in files:\n",
        "                    # This will include the full directory structure\n",
        "                    full_path = os.path.join(root, file)\n",
        "                    relative_path = os.path.relpath(full_path, os.path.dirname(file_or_folder))\n",
        "                    zipf.write(full_path, arcname=relative_path)\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "# Define the timezone\n",
        "sgt = pytz.timezone('Asia/Singapore')\n",
        "\n",
        "# get current date and time\n",
        "now = datetime.now(sgt)\n",
        "\n",
        "# format as a string in YYMMDDHHMM format\n",
        "time_now = \"20\" + now.strftime(\"%y%m%d%H%M\") + \"_\"\n",
        "\n",
        "devtest = str(test_ids + validation_ids)\n",
        "with open(time_now +'devtest.txt', 'w') as f:\n",
        "    # Write the string to the file\n",
        "    f.write(devtest)\n",
        "time_take = time.time() - start\n",
        "\n",
        "print(f\"Completed data processing for train/validation/test set, time taken {time_take}\")\n",
        "\n",
        "# Saving part\n",
        "filenames_and_folders = ['./p1_X_tensor.pt', './p1_Y_tensor.pt', './validation_dataset.csv', './p2_X_tensor.pt',\n",
        "         './p2_Y_tensor.pt', './train_dataset.csv', './p3_X_tensor.pt', './p3_Y_tensor.pt', './test_dataset.csv', './devtest.txt'\n",
        "         ]\n",
        "\n",
        "bef_stand_heatmap = ['./before_stand_heatmap/']\n",
        "final_heatmap = ['./final_heatmap/']\n",
        "\n",
        "zip_name_dataset = time_now + 'dataset.zip'\n",
        "zip_name_befstand_heatmap = time_now + 'befstand_heatmap.zip'\n",
        "zip_name_final_heatmap = time_now + 'final.zip'\n",
        "\n",
        "zip_files_and_folders(filenames_and_folders, zip_name_dataset)\n",
        "zip_folders(bef_stand_heatmap, zip_name_befstand_heatmap)\n",
        "zip_folders(final_heatmap, zip_name_final_heatmap)\n",
        "\n",
        "# print the result\n",
        "print(f\"Preparing dataset done at {time_now}\")\n",
        "\n",
        "# Define source and destination paths\n",
        "src_path = './' + zip_name_dataset\n",
        "befheatmap_src_path = './' +  zip_name_befstand_heatmap\n",
        "finalheatmap_src_path = './' +  zip_name_final_heatmap\n",
        "\n",
        "dest_folder = '/content/gdrive/MyDrive/IVODData'  # replace with your folder\n",
        "\n",
        "# Make sure that the destination folder exists\n",
        "os.makedirs(dest_folder, exist_ok=True)\n",
        "\n",
        "# Copy the file and print output (MUST SEE THIS PRINT TO COMPLETE PROCESSING!!)\n",
        "shutil.copy(src_path, dest_folder)\n",
        "print(f'File {src_path} has been copied to: {dest_folder}')\n",
        "shutil.copy(befheatmap_src_path, dest_folder)\n",
        "print(f'File {befheatmap_src_path} has been copied to: {dest_folder}')\n",
        "shutil.copy(finalheatmap_src_path, dest_folder)\n",
        "print(f'File {finalheatmap_src_path} has been copied to: {dest_folder}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Eyj6HE631JkY",
        "outputId": "a4541b7d-0536-4e82-d91b-693fa8df2838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed data processing for train/validation/test set, time taken 41561.11449289322\n",
            "Preparing dataset done at 202307111319_\n",
            "File ./202307111319_dataset.zip has been copied to: /content/gdrive/MyDrive/IVODData\n",
            "File ./202307111319_befstand_heatmap.zip has been copied to: /content/gdrive/MyDrive/IVODData\n",
            "File ./202307111319_final.zip has been copied to: /content/gdrive/MyDrive/IVODData\n"
          ]
        }
      ],
      "source": [
        "# Old file names copy\n",
        "\n",
        "def zip_files_and_folders(filenames_and_folders, zip_name):\n",
        "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for file_or_folder in filenames_and_folders:\n",
        "            if os.path.isfile(file_or_folder):\n",
        "                # If it is a file, write it directly.\n",
        "                zipf.write(file_or_folder)\n",
        "            elif os.path.isdir(file_or_folder):\n",
        "                # If it is a directory, write all its contents.\n",
        "                for root, dirs, files in os.walk(file_or_folder):\n",
        "                    for file in files:\n",
        "                        # This will include the full directory structure\n",
        "                        full_path = os.path.join(root, file)\n",
        "                        relative_path = os.path.relpath(full_path, os.path.dirname(file_or_folder))\n",
        "                        zipf.write(full_path, arcname=relative_path)\n",
        "            else:\n",
        "                print(f\"{file_or_folder} is neither a valid file nor a folder.\")\n",
        "\n",
        "def zip_folders(filenames_and_folders, zip_name):\n",
        "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for file_or_folder in filenames_and_folders:\n",
        "            # If it is a directory, write all its contents.\n",
        "            for root, dirs, files in os.walk(file_or_folder):\n",
        "                for file in files:\n",
        "                    # This will include the full directory structure\n",
        "                    full_path = os.path.join(root, file)\n",
        "                    relative_path = os.path.relpath(full_path, os.path.dirname(file_or_folder))\n",
        "                    zipf.write(full_path, arcname=relative_path)\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "# Define the timezone\n",
        "sgt = pytz.timezone('Asia/Singapore')\n",
        "\n",
        "# get current date and time\n",
        "now = datetime.now(sgt)\n",
        "\n",
        "# format as a string in YYMMDDHHMM format\n",
        "time_now = \"20\" + now.strftime(\"%y%m%d%H%M\") + \"_\"\n",
        "\n",
        "devtest = str(test_ids + validation_ids)\n",
        "with open(time_now +'devtest.txt', 'w') as f:\n",
        "    # Write the string to the file\n",
        "    f.write(devtest)\n",
        "time_take = time.time() - start\n",
        "\n",
        "print(f\"Completed data processing for train/validation/test set, time taken {time_take}\")\n",
        "\n",
        "# Saving part\n",
        "filenames_and_folders = ['./train_X_tensor.pt', './train_Y_tensor.pt', './validation_dataset.csv', './validation_X_tensor.pt',\n",
        "         './validation_Y_tensor.pt', './train_dataset.csv', './test_X_tensor.pt', './test_Y_tensor.pt', './test_dataset.csv', './devtest.txt'\n",
        "         ]\n",
        "\n",
        "bef_stand_heatmap = ['./before_stand_heatmap/']\n",
        "final_heatmap = ['./final_heatmap/']\n",
        "\n",
        "zip_name_dataset = time_now + 'dataset.zip'\n",
        "zip_name_befstand_heatmap = time_now + 'befstand_heatmap.zip'\n",
        "zip_name_final_heatmap = time_now + 'final.zip'\n",
        "\n",
        "zip_files_and_folders(filenames_and_folders, zip_name_dataset)\n",
        "zip_folders(bef_stand_heatmap, zip_name_befstand_heatmap)\n",
        "zip_folders(final_heatmap, zip_name_final_heatmap)\n",
        "\n",
        "# print the result\n",
        "print(f\"Preparing dataset done at {time_now}\")\n",
        "\n",
        "# Define source and destination paths\n",
        "src_path = './' + zip_name_dataset\n",
        "befheatmap_src_path = './' +  zip_name_befstand_heatmap\n",
        "finalheatmap_src_path = './' +  zip_name_final_heatmap\n",
        "\n",
        "dest_folder = '/content/gdrive/MyDrive/IVODData'  # replace with your folder\n",
        "\n",
        "# Make sure that the destination folder exists\n",
        "os.makedirs(dest_folder, exist_ok=True)\n",
        "\n",
        "# Copy the file and print output (MUST SEE THIS PRINT TO COMPLETE PROCESSING!!)\n",
        "shutil.copy(src_path, dest_folder)\n",
        "print(f'File {src_path} has been copied to: {dest_folder}')\n",
        "shutil.copy(befheatmap_src_path, dest_folder)\n",
        "print(f'File {befheatmap_src_path} has been copied to: {dest_folder}')\n",
        "shutil.copy(finalheatmap_src_path, dest_folder)\n",
        "print(f'File {finalheatmap_src_path} has been copied to: {dest_folder}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2Ctk1bMTVeg"
      },
      "source": [
        "For future"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "r-cJdB92TV--",
        "outputId": "d4d53bbd-3821-4eff-acbf-7dc5128988c9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'import numpy as np\\n\\ndef upscale_array_2d(input_array, output_size, noise_scale=0.1):\\n    input_size = input_array.shape[0]\\n    repeat_times = output_size // input_size\\n    remaining = output_size % input_size\\n\\n    # create output array by repeating each row of the input array\\n    output_array = np.repeat(input_array, repeat_times, axis=0)\\n\\n    # if the output size is not a multiple of the input size,\\n    # we need to append some rows to the output array\\n    if remaining != 0:\\n        # add rows from the input array to the output array\\n        # until it reaches the desired size\\n        output_array = np.concatenate((output_array, np.zeros((remaining, 3))), axis=0)\\n\\n    # add noise to output array\\n    noise = np.random.normal(0, noise_scale, output_array.shape)\\n    output_array = output_array + noise\\n\\n    return output_array\\n\\n# testing the function\\ninput_array = np.array([[1,2,3],[2,3,4],[3,4,5],[4,5,6]])\\noutput_size = 38\\n\\nimport time\\nstart = time.time()\\nprint(upscale_array_2d(input_array, output_size, 0))\\ntime_take = time.time() - start\\n\\nprint(time_take)'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The upscale method can be used for padding\n",
        "\n",
        "'''import numpy as np\n",
        "\n",
        "def upscale_array_2d(input_array, output_size, noise_scale=0.1):\n",
        "    input_size = input_array.shape[0]\n",
        "    repeat_times = output_size // input_size\n",
        "    remaining = output_size % input_size\n",
        "\n",
        "    # create output array by repeating each row of the input array\n",
        "    output_array = np.repeat(input_array, repeat_times, axis=0)\n",
        "\n",
        "    # if the output size is not a multiple of the input size,\n",
        "    # we need to append some rows to the output array\n",
        "    if remaining != 0:\n",
        "        # add rows from the input array to the output array\n",
        "        # until it reaches the desired size\n",
        "        output_array = np.concatenate((output_array, np.zeros((remaining, 3))), axis=0)\n",
        "\n",
        "    # add noise to output array\n",
        "    noise = np.random.normal(0, noise_scale, output_array.shape)\n",
        "    output_array = output_array + noise\n",
        "\n",
        "    return output_array\n",
        "\n",
        "# testing the function\n",
        "input_array = np.array([[1,2,3],[2,3,4],[3,4,5],[4,5,6]])\n",
        "output_size = 38\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "print(upscale_array_2d(input_array, output_size, 0))\n",
        "time_take = time.time() - start\n",
        "\n",
        "print(time_take)'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}